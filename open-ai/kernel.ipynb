{
  "cells": [
    {
      "metadata": {
        "_uuid": "a99e1f75bc817bf7fbb6f9b9a0bdca6fe81a108d"
      },
      "cell_type": "markdown",
      "source": "# Projet commencé le 04/02/2019\n\nLien utiles :\n* [https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8](http://)\n* [https://arxiv.org/pdf/1807.07327.pdf](http://) \n* [https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer](http://)"
    },
    {
      "metadata": {
        "_uuid": "f8a17a649d4c4044af9cb18c4b62cf6c54d881ba"
      },
      "cell_type": "markdown",
      "source": "## Imports :"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Algèbre linéaire\nimport numpy as np\n\n# Data processing, CSV\nimport pandas as pd\n\n# Division de float\nfrom __future__ import division\n\n# Type Image\nfrom PIL import Image\n\n# Image plot\nimport matplotlib.pyplot as plt\n\n# Récupère l'image d'une requête\nfrom io import BytesIO\n\n# Requête sur URL\nimport requests\n\n# Permet l'accès à Big Querry\nimport bq_helper\n\n# Mélange du dataset et séparation train/test\nfrom sklearn.model_selection import train_test_split\n\n# Partie machine learning\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Convolution2D, Flatten, MaxPooling2D, Dropout, Activation\nfrom keras.utils import to_categorical\nfrom keras.models import load_model\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\n\n# Utilitaire\nimport os",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "458d83c8a1bb5084d6aaa1b72273e83c1f9664bc",
        "_kg_hide-input": false,
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "# INIT\nopen_images = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\", dataset_name=\"open_images\")",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using Kaggle's public dataset BigQuery integration.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8040e9abddde8aab94858acb127e81d57a53cba8"
      },
      "cell_type": "markdown",
      "source": "## Définitions de fonctions :"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cab4e20497b91f933f67ee646cc8e2b94ad9d8c7",
        "_kg_hide-input": false,
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "# Return l'image trouvable sur une URL\ndef images_from_url(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content))",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cec19739ea356e27512b0b1a5927512b5ba89d00",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Return le résultat de la querry\ndef querry_dataset(size):\n    print(\"Loading dataset...\")\n    subQueryImages = \"\"\"\n    (SELECT image_id, thumbnail_300k_url\n    FROM `bigquery-public-data.open_images.images`\n    WHERE thumbnail_300k_url IS NOT NULL)\n    \"\"\"\n\n    subQueryBox = \"\"\"\n    (SELECT image_id, label_name, x_min, x_max, y_min, y_max\n    FROM `bigquery-public-data.open_images.annotations_bbox`\n    ORDER BY image_id\n    LIMIT \"\"\" + str(size) + \"\"\")\n    \"\"\"\n\n    subQueryWord = \"\"\"\n    (SELECT label_name, label_display_name\n    FROM `bigquery-public-data.open_images.dict`)\n    \"\"\"\n\n    labelQuery = \"\"\"\n    (SELECT ROW_NUMBER() OVER (ORDER BY wrd.label_name) - 1 AS row, wrd.label_name\n    FROM (SELECT DISTINCT(label_name) FROM \"\"\" + subQueryBox + \"\"\") lab\n    JOIN \"\"\" + subQueryWord + \"\"\" wrd ON lab.label_name = wrd.label_name)\"\"\"\n\n    mainQuery = \"\"\"\n    SELECT img.thumbnail_300k_url, wrd.label_display_name, box.label_name, idx.row, box.x_min, box.x_max, box.y_min, box.y_max\n    FROM \"\"\" + subQueryImages + \"\"\" img\n    INNER JOIN \"\"\" + subQueryBox + \"\"\" box ON img.image_id = box.image_id\n    INNER JOIN \"\"\" + subQueryWord + \"\"\" wrd ON box.label_name = wrd.label_name\n    INNER JOIN \"\"\" + labelQuery + \"\"\" idx ON idx.label_name = wrd.label_name\n    ORDER BY img.thumbnail_300k_url\n    \"\"\"\n    print(\"Dataset loaded\")\n    return open_images.query_to_pandas_safe(mainQuery)",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66b4c9aac8abc699d359b842177b2a7b2231663e"
      },
      "cell_type": "code",
      "source": "# Charge les images et les labels(bbox et mots) dans la ram\ndef load_data(data_start, data_length, image_size):\n    print(\"Loading images from URL... (From \" + str(data_start) + \" to \" + str(data_start + data_length) + \" with a image size of \" + str(image_size) + \")\")\n    \n    for data_ex in range (data_start, data_start + data_length):\n        # Récupere l'image et la resize\n        image = images_from_url(dataset.thumbnail_300k_url[data_ex])\n        image = np.array(image.resize((image_size, image_size)))\n        \n        # Fait un one-hot pour les labels \"mots\"\n        if (len(image.shape) == 3): # Filtre les images qui sont pas suppr\n            features[data_ex - data_start] = image/255\n            labels_word[data_ex - data_start][dataset.row[data_ex]] = 1\n            labels_bbox[data_ex - data_start] = [dataset.x_min[data_ex], dataset.y_min[data_ex], dataset.x_max[data_ex], dataset.y_max[data_ex]]\n            labels.append([labels_word[data_ex - data_start], [dataset.x_min[data_ex], dataset.y_min[data_ex], dataset.x_max[data_ex], dataset.y_max[data_ex]]])\n            \n    lables = np.array(labels)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.1, random_state=42)\n\n    y_train = np.array(y_train)\n    y_valid = np.array(y_valid)\n    print(y_train.shape)\n    \n    y_train_word = [y_train[i][0] for i in range(0, y_train.shape[0])]\n    y_train_word = np.array(y_train_word)\n    print(y_train_word.shape)\n    \n    y_train_bbox = y_train[:, 1]\n    y_valid_word = y_valid[:, 0]\n    y_valid_bbox = y_valid[:, 1]\n    \n    print(\"Image loaded\")\n    \n    return [X_train, X_valid, y_train_word, y_train_bbox, y_valid_word, y_valid_bbox]\n            ",
      "execution_count": 135,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a514a4559ca1dde05b3f0da91b096cbb99b17909"
      },
      "cell_type": "code",
      "source": "def crop_image(image, image_size, x_min, y_min, x_max, y_max):\n    image.resize((image_size, image_size))\n    box_rectangle = (image_size*x_min, image_size*y_min, image_size*x_max, image_size*y_max)\n    image = image.crop(box_rectangle)\n    image = np.array(image)",
      "execution_count": 136,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "216ee568928dafe3576a01050a4d96ce44b44fb5"
      },
      "cell_type": "code",
      "source": "def create_anchor(x, y, w, h):\n    return [x-(w/2), y-(h/2), x+(w/2), y+(h/2)]",
      "execution_count": 137,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "890508378ba6bf14106718d0ba039d4cee8c9c31"
      },
      "cell_type": "code",
      "source": "dataset_size = 100\nsub_dataset_size = 100\nimage_size = 224\n\nfeatures = np.ndarray(shape=(sub_dataset_size, image_size, image_size, 3), dtype=float)\nlabels_word = np.zeros(shape=(sub_dataset_size, 600), dtype=float)\nlabels_bbox = np.ndarray(shape=(sub_dataset_size, 4), dtype=float)\nlabels = []\n\ndataset = querry_dataset(dataset_size)",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading dataset...\nDataset loaded\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a79fe886b39abe01ac598185b452c585815e122d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X_train, X_valid, y_train_word, y_train_bbox, y_valid_word, y_valid_bbox = load_data(0, 100, 224)\n\n\n\"\"\"X_train = data[0]\nX_valid = data[1]\ny_train_word = data[2]\ny_train_bbox = data[3]\ny_valid_word = data[4]\ny_valid_bbox = data[5]\"\"\"",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading images from URL... (From 0 to 100 with a image size of 224)\n(90, 2)\n(90, 600)\nImage loaded\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 139,
          "data": {
            "text/plain": "'X_train = data[0]\\nX_valid = data[1]\\ny_train_word = data[2]\\ny_train_bbox = data[3]\\ny_valid_word = data[4]\\ny_valid_bbox = data[5]'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e44dedbb5b11b3770fab59e7b482dc2a0dbf767c",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "053b2c7a0236e804ab2a047afa9e7be40edb9143"
      },
      "cell_type": "code",
      "source": "#Instantiate an empty model\nmodel = Sequential()\n\n# 1st Convolutional Layer\nmodel.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\nmodel.add(Activation('relu'))\n# Max Pooling\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n\n# 2nd Convolutional Layer\nmodel.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\n# Max Pooling\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n\n# 3rd Convolutional Layer\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\n\n# 4th Convolutional Layer\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\n\n# 5th Convolutional Layer\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\nmodel.add(Activation('relu'))\n# Max Pooling\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n\n# Passing it to a Fully Connected layer\nmodel.add(Flatten())\n# 1st Fully Connected Layer\nmodel.add(Dense(4096, input_shape=(224*224*3,)))\nmodel.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nmodel.add(Dropout(0.4))\n\n# 2nd Fully Connected Layer\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\n# Add Dropout\nmodel.add(Dropout(0.4))\n\n# 3rd Fully Connected Layer\nmodel.add(Dense(1000))\nmodel.add(Activation('relu'))\n# Add Dropout\nmodel.add(Dropout(0.4))\n\n# Output Layer\nmodel.add(Dense(600))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_86 (Conv2D)           (None, 54, 54, 96)        34944     \n_________________________________________________________________\nactivation_154 (Activation)  (None, 54, 54, 96)        0         \n_________________________________________________________________\nmax_pooling2d_52 (MaxPooling (None, 27, 27, 96)        0         \n_________________________________________________________________\nconv2d_87 (Conv2D)           (None, 17, 17, 256)       2973952   \n_________________________________________________________________\nactivation_155 (Activation)  (None, 17, 17, 256)       0         \n_________________________________________________________________\nmax_pooling2d_53 (MaxPooling (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_88 (Conv2D)           (None, 6, 6, 384)         885120    \n_________________________________________________________________\nactivation_156 (Activation)  (None, 6, 6, 384)         0         \n_________________________________________________________________\nconv2d_89 (Conv2D)           (None, 4, 4, 384)         1327488   \n_________________________________________________________________\nactivation_157 (Activation)  (None, 4, 4, 384)         0         \n_________________________________________________________________\nconv2d_90 (Conv2D)           (None, 2, 2, 256)         884992    \n_________________________________________________________________\nactivation_158 (Activation)  (None, 2, 2, 256)         0         \n_________________________________________________________________\nmax_pooling2d_54 (MaxPooling (None, 1, 1, 256)         0         \n_________________________________________________________________\nflatten_18 (Flatten)         (None, 256)               0         \n_________________________________________________________________\ndense_69 (Dense)             (None, 4096)              1052672   \n_________________________________________________________________\nactivation_159 (Activation)  (None, 4096)              0         \n_________________________________________________________________\ndropout_52 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_70 (Dense)             (None, 4096)              16781312  \n_________________________________________________________________\nactivation_160 (Activation)  (None, 4096)              0         \n_________________________________________________________________\ndropout_53 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_71 (Dense)             (None, 1000)              4097000   \n_________________________________________________________________\nactivation_161 (Activation)  (None, 1000)              0         \n_________________________________________________________________\ndropout_54 (Dropout)         (None, 1000)              0         \n_________________________________________________________________\ndense_72 (Dense)             (None, 600)               600600    \n_________________________________________________________________\nactivation_162 (Activation)  (None, 600)               0         \n=================================================================\nTotal params: 28,638,080\nTrainable params: 28,638,080\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "832c2f7972d8113560af9b9d36c4881429db3a94"
      },
      "cell_type": "code",
      "source": "model.fit(X_train, y_train_word, nb_epoch=5, batch_size=1)",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  \"\"\"Entry point for launching an IPython kernel.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Epoch 1/5\n90/90 [==============================] - 5s 52ms/step - loss: 3.8174 - acc: 0.0889\nEpoch 2/5\n90/90 [==============================] - 4s 49ms/step - loss: 3.7835 - acc: 0.1000\nEpoch 3/5\n90/90 [==============================] - 4s 49ms/step - loss: 3.5568 - acc: 0.1333\nEpoch 4/5\n90/90 [==============================] - 4s 49ms/step - loss: 3.5465 - acc: 0.0889\nEpoch 5/5\n90/90 [==============================] - 4s 49ms/step - loss: 3.4970 - acc: 0.1222\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 145,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f77da3e94e0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28fa5b778465a9a098a0b3702a77d7c01b59981c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23b1335e7379b53b39ff55a6e7abdb92caacc10a",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Image -> Convolution\n# Convolution + Chaques anchor -> Croped Image\n# Croped Image -> RPN (Objet ? Oui/Non + Vrai Coo) (Pour backprop, check toutes les BBOXs de l'image puis IoU)\n# Si object, on crop sur les vrais coos -> Classifier\n\nmodel = Sequential()\n\nmodel.add(Conv2D(96, kernel_size=(11, 11), strides=4, activation='relu',input_shape=(image_size_x,image_size_y,3)), name='input conv')\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(256, kernel_size=(5, 5),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(384, kernel_size=(3, 3),activation='relu'))\nmodel.add(Conv2D(384, kernel_size=(3, 3),activation='relu'))\nmodel.add(Conv2D(256, kernel_size=(3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), name='output conv'))\n\n#https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dense(600, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n#model.save('my_model.h5')\n\nfor sub_dataset_ex in range (0, int(dataset_size/sub_dataset_size)):\n    load_data(sub_dataset_ex*sub_dataset_size, sub_dataset_size, image_size)\n    X_train, X_valid, y_train, y_valid = train_test_split(features_croped, labels_croped, test_size=0.1, random_state=42)\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=4)\n    #model.save('my_model.h5')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bedc88cf2e4b51f4168212ba612d4039f4e1564"
      },
      "cell_type": "code",
      "source": "#model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=128)\n\n#print(y_train[1])\n\nfor i in range (0, sub_dataset_size):\n    plt.imshow(X_train[i])a\n    plt.show()\n    \n\"\"\"# Récupere la sous image\n        width, height = image.size\n        box_rectangle = (width*dataset.x_min[data_ex], height*dataset.y_min[data_ex], width*dataset.x_max[data_ex], height*dataset.y_max[data_ex])\n        image = image.crop(box_rectangle)\n        image = np.array(image.resize((image_size_x, image_size_y)))\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "653f585cdead148d4cb3608a21be667460fb23b3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}